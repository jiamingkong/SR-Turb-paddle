{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 飞桨论文复现赛 第七期 Unsupervised deep learning for super-resolution reconstruction of turbulence\n",
    "\n",
    "**什么是惊喜队**\n",
    "\n",
    "## 简介\n",
    "\n",
    "本项目主要是用以复现[Unsupervised deep learning for super-resolution reconstruction of turbulence](https://arxiv.org/abs/2007.15324)文章。原文作者在github中[开放了部分的源代码](https://github.com/HyojinKim-github/SR-Turb-CycleGAN)，但源代码是tensorflow构建的，同时代码中有少许不合理的地方，修复后也无法实现原文中的精度。\n",
    "\n",
    "本文使用了Paddle重现该文章的工作，附带了一部分[Johns Hopkins Turbulence Databases](http://turbulence.pha.jhu.edu/datasets.aspx)的数据用作该项目的可视化验证。本项目与原始的代码实现的差异主要有如下：\n",
    "\n",
    "- 扩大了网络的规模；\n",
    "- 引入了residual connection；\n",
    "- 对湍流速度场的数据进行了缩放，并让模型最后输出可以tanh激活；\n",
    "- 重现了原文没有实现的数据IO等训练管线\n",
    "\n",
    "\n",
    "## 原文介绍\n",
    "\n",
    "湍流数值模拟结果的超分辨率一直是近年来的研究热点，通过深度学习模型将低分辨率的结果细化成高分辨率的结果可以大幅度地节省传统数值模拟的计算耗时。但是之前的研究都是使用了监督学习的方式，使用成对的高低分辨率数据进行训练，在一些流体力学的应用场景里面（例如Large eddy simulation），这样的成对数据可能不好获得。所以本文作者提出了使用CycleGAN进行无监督的方式来训练湍流的超分模型。\n",
    "\n",
    "本文提出的超分辨率模型是基于CycleGAN训练的，训练的时候可以输入*不成对的高低分辨率湍流数值模拟结果*，假设我们称低分辨率数据为LR，高分辨率为HR，则该框架同时训练以下四个模型；\n",
    "\n",
    "- $G(LR) \\rightarrow \\widehat{HR}$：超分辨率模型\n",
    "- $F(HR) \\rightarrow \\widehat{LR}$：降采样模型\n",
    "- $DX(LR) \\rightarrow (0,1)$：低分辨率数据的辨别器\n",
    "- $DY(HR) \\rightarrow (0,1)$：高分辨率数据的辨别器\n",
    "\n",
    "在训练的时候，我们将会同时输入不成对的低分辨率数据X，高分辨率数据Y：\n",
    "\n",
    "1. 计算$\\widehat{Y} = G(X), \\widehat{X} = F(Y)$，\n",
    "2. 计算辨别器损失：Loss_DX = DX(X) - DX(F(Y))， Loss_DY = DY(Y) - DY(G(X))\n",
    "3. 计算循环损失: Loss_cycle = (Y - G(F(Y)))^2 + (X - F(G(X))^2\n",
    "4. 计算生成器的损失：Loss_G = Loss_cycle + DY(G(X))， Loss_F = Loss_cycle + DX(F(Y))\n",
    "4. 完成梯度的backprop\n",
    "\n",
    "## 项目架构简单说明\n",
    "\n",
    "- main.ipynb：本notebook，展示了项目的基础结构和一些流程；\n",
    "- models/generators.py, models/discriminators.py：生成和辨别模型的定义\n",
    "- utils/\n",
    "    - loss.py：定义了损失的计算方法\n",
    "    - dataloader.py：定义了数据的IO管线\n",
    "    - functions.py：一些常用的函数，包括2X和0.5X层的定义\n",
    "\n",
    "## 运行方法\n",
    "\n",
    "直接点击“运行全部”，将会加载已经训练好的权重进行超分辨率重建；\n",
    "\n",
    "在“开始训练”章节将两行代码取消注释，将会训练一个全新的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集 JHTDB\n",
    "\n",
    "我们附带了用来验证训练管线和结果的可视化，按照原文描述，该超分辨率模型尝试复现的是速度场，对应的数据库名称是[Forced isotropic turbulence](http://turbulence.pha.jhu.edu/Forced_isotropic_turbulence.aspx)。JHTDB提供的是$1024^3$格点里面的3维速度场数据(u,v,w)。每次训练时，我们会固定z = 0，在x-y平面中截取$128^2$的数据(128,128,3)，而低分辨率数据则通过在高分辨率数据中取2\\*2的区块进行平均得出(32,32,3)。该项目已经挂载了相应的数据集，可以直接看到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 展示已经挂载的JHTDB cutout\n",
    "!ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First things first\n",
    "\n",
    "依赖项目安装：\n",
    "- h5py 用以读取cutout\n",
    "- tqdm 用以展示训练进度条\n",
    "\n",
    "按照说明安装在自建的路径，以后直接挂载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p external-libraries\n",
    "!pip install tqdm -t external-libraries --upgrade\n",
    "!pip install h5py -t external-libraries --upgrade\n",
    "\n",
    "!mkdir -p saved_models\n",
    "!mkdir -p log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T10:05:03.442097Z",
     "iopub.status.busy": "2022-10-07T10:05:03.441281Z",
     "iopub.status.idle": "2022-10-07T10:05:03.444866Z",
     "shell.execute_reply": "2022-10-07T10:05:03.444252Z",
     "shell.execute_reply.started": "2022-10-07T10:05:03.442063Z"
    }
   },
   "source": [
    "## 代码主体\n",
    "\n",
    "### 超参数设定\n",
    "\n",
    "以下是超参数的设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle as pd\n",
    "import paddle.nn as nn\n",
    "from paddle.io import DataLoader\n",
    "from models.discriminators import DiscriminatorX, DiscriminatorY\n",
    "from models.generators import GeneratorG, GeneratorF\n",
    "from utils.dataloader import TurbulenceDataset, get_energy\n",
    "from utils.loss import wgp_slope_condition, identity_loss, cycle_consistency_loss\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 学习率\n",
    "LEARNING_RATE = 1e-4\n",
    "# 训练轮次\n",
    "EPOCHS = 50\n",
    "# 梯度惩罚的损失乘数\n",
    "LAMBDA_GRAD = 10\n",
    "# 循环一致性的损失乘数\n",
    "LAMBDA_CYCLE = 200\n",
    "# 批次大小\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# 用来计算cross entropy的标靶\n",
    "ALL_TRUE = pd.to_tensor([1] * BATCH_SIZE, dtype=\"float32\")\n",
    "ALL_FALSE = pd.to_tensor([0] * BATCH_SIZE, dtype=\"float32\")\n",
    "\n",
    "# 设定日志\n",
    "from datetime import datetime\n",
    "from visualdl import LogWriter\n",
    "RUN = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "writer = LogWriter(logdir=f\"./log/train_{RUN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器和鉴别器相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_step(X: pd.Tensor, Y: pd.Tensor, genG: nn.Layer, genF: nn.Layer) -> tuple:\n",
    "    \"\"\"\n",
    "    给定一对高低分辨率的湍流数据，使用generatorG和generatorF完成对应的生成步骤：\n",
    "    genG(X) -> Y_predict: 生成器G将低分辨率的X转换为高分辨率的Y_predict\n",
    "    genG(X_predict) -> Y_cycle: 生成器G将低分辨率的X_predict转换为高分辨率的Y_cycle\n",
    "\n",
    "    genF(Y) -> X_predict: 生成器F将高分辨率的Y转换为低分辨率的X_predict\n",
    "    genF(Y_predict) -> X_cycle: 生成器F将高分辨率的Y_predict转换为低分辨率的X_cycle\n",
    "\n",
    "    最后组装成一个字典返回\n",
    "    \"\"\"\n",
    "    Y_predict = genG(X)\n",
    "    X_cycle = genF(Y_predict)\n",
    "\n",
    "    X_predict = genF(Y)\n",
    "    Y_cycle = genG(X_predict)\n",
    "\n",
    "    return {\n",
    "        \"X_predict\": X_predict,\n",
    "        \"Y_predict\": Y_predict,\n",
    "        \"X_cycle\": X_cycle,\n",
    "        \"Y_cycle\": Y_cycle,\n",
    "        \"X_real\": X,\n",
    "        \"Y_real\": Y,\n",
    "    }\n",
    "\n",
    "\n",
    "def discriminate_step(gen_dict: dict, discX: nn.Layer, discY: nn.Layer) -> dict:\n",
    "    \"\"\"\n",
    "    给定生成器生成的数据，使用discriminatorX和discriminatorY完成对应的鉴别步骤：\n",
    "    discX(X_predict) -> X_predict_score: 鉴别器X对低分辨率的X_predict进行鉴别\n",
    "    discY(Y_predict) -> Y_predict_score: 鉴别器Y对高分辨率的Y_predict进行鉴别\n",
    "    discX(X_real) -> X_real_score: 鉴别器X对低分辨率的X_real进行鉴别\n",
    "    discY(Y_real) -> Y_real_score: 鉴别器Y对高分辨率的Y_real进行鉴别\n",
    "    最后组装成一个字典返回\n",
    "    \"\"\"\n",
    "    discX_real = nn.functional.binary_cross_entropy_with_logits(\n",
    "        discX(gen_dict[\"X_real\"]).flatten(), ALL_TRUE\n",
    "    )\n",
    "    discX_predict = nn.functional.binary_cross_entropy_with_logits(\n",
    "        discX(gen_dict[\"X_predict\"]).flatten(), ALL_FALSE\n",
    "    )\n",
    "    discY_real = nn.functional.binary_cross_entropy_with_logits(\n",
    "        discY(gen_dict[\"Y_real\"]).flatten(), ALL_TRUE\n",
    "    )\n",
    "    discY_predict = nn.functional.binary_cross_entropy_with_logits(\n",
    "        discY(gen_dict[\"Y_predict\"]).flatten(), ALL_FALSE\n",
    "    )\n",
    "    return {\n",
    "        \"discX_real\": discX_real,\n",
    "        \"discX_predict\": discX_predict,\n",
    "        \"discY_real\": discY_real,\n",
    "        \"discY_predict\": discY_predict,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练的步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    X: pd.Tensor,\n",
    "    Y: pd.Tensor,\n",
    "    genG: nn.Layer,\n",
    "    genF: nn.Layer,\n",
    "    discX: nn.Layer,\n",
    "    discY: nn.Layer,\n",
    "    lambda_cycle: float,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    给定一对高低分辨率的湍流数据，使用generatorG和generatorF完成对应的生成步骤\n",
    "    \"\"\"\n",
    "    gen_dict = generate_step(X, Y, genG, genF)\n",
    "    disc_dict = discriminate_step(gen_dict, discX, discY)\n",
    "    wgp_dict = wgp_slope_condition(gen_dict, discX, discY)\n",
    "    cycle_dict = cycle_consistency_loss(gen_dict, lambda_cycle)\n",
    "\n",
    "    # discriminator loss\n",
    "    DX_loss = (\n",
    "        disc_dict[\"discX_predict\"] + disc_dict[\"discX_real\"]\n",
    "        + LAMBDA_GRAD * wgp_dict[\"gradient_penalty_X\"]\n",
    "    )\n",
    "    DY_loss = (\n",
    "        disc_dict[\"discY_predict\"] + disc_dict[\"discY_real\"]\n",
    "        + LAMBDA_GRAD * wgp_dict[\"gradient_penalty_Y\"]\n",
    "    )\n",
    "    # cycle consistency loss\n",
    "    cycle_loss = cycle_dict[\"X_cycle_loss\"] + cycle_dict[\"Y_cycle_loss\"]\n",
    "    # generator loss\n",
    "    G_loss = -disc_dict[\"discY_predict\"] + LAMBDA_CYCLE * cycle_loss\n",
    "    F_loss = -disc_dict[\"discX_predict\"] + LAMBDA_CYCLE * cycle_loss\n",
    "    \n",
    "    return {\"DX_loss\": DX_loss, \"DY_loss\": DY_loss, \"G_loss\": G_loss, \"F_loss\": F_loss, \"MSE\": pd.mean(pd.square(Y-gen_dict[\"Y_predict\"]))}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    data_loader,\n",
    "    genG,\n",
    "    genF,\n",
    "    discX,\n",
    "    discY,\n",
    "    genG_opt,\n",
    "    genF_opt,\n",
    "    discX_opt,\n",
    "    discY_opt,\n",
    "    epoch,\n",
    "    lambda_cycle,\n",
    "    total_step\n",
    "):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    \"\"\"\n",
    "    pbar = tqdm(enumerate(data_loader), total=int(10000/BATCH_SIZE))\n",
    "    for batch_id, data in pbar:\n",
    "        X, Y = data\n",
    "        X = pd.to_tensor(X)\n",
    "        Y = pd.to_tensor(Y)\n",
    "        loss = train_step(X, Y, genG, genF, discX, discY, lambda_cycle)\n",
    "        DX_loss = loss[\"DX_loss\"]\n",
    "        DY_loss = loss[\"DY_loss\"]\n",
    "        G_loss = loss[\"G_loss\"]\n",
    "        F_loss = loss[\"F_loss\"]\n",
    "        mse = loss[\"MSE\"]\n",
    "        # mse.backward(retain_graph = True)\n",
    "        G_loss.backward(retain_graph = True)\n",
    "        F_loss.backward(retain_graph = True)\n",
    "        DX_loss.backward(retain_graph = True)\n",
    "        DY_loss.backward(retain_graph = True)\n",
    "        genG_opt.minimize(G_loss)\n",
    "        genF_opt.minimize(F_loss)\n",
    "        discX_opt.minimize(DX_loss)\n",
    "        discY_opt.minimize(DY_loss)\n",
    "        genG_opt.clear_grad()\n",
    "        genF_opt.clear_grad()\n",
    "        discX_opt.clear_grad()\n",
    "        discY_opt.clear_grad()\n",
    "        total_step += 1\n",
    "        \n",
    "        \n",
    "        writer.add_scalar(tag=\"DX_loss\", step=total_step, value=DX_loss.item())\n",
    "        writer.add_scalar(tag=\"DY_loss\", step=total_step, value=DY_loss.item())\n",
    "        writer.add_scalar(tag=\"G_loss\", step=total_step, value=G_loss.item())\n",
    "        writer.add_scalar(tag=\"F_loss\", step=total_step, value=F_loss.item())\n",
    "        writer.add_scalar(tag=\"MSE\", step=total_step, value=loss[\"MSE\"].item())\n",
    "        pbar.set_description(\n",
    "            f\"Epoch {epoch} DX: {DX_loss.item():.3f} DY: {DY_loss.item():.3f} G: {G_loss.item():.3f} F: {F_loss.item(): .3f}, MSE: {mse.item():.3f}\"\n",
    "        )\n",
    "    # sample an image\n",
    "        if total_step % 100 == 0:\n",
    "            Y_hat = genG(X[0:1,:,:,:])\n",
    "            writer.add_image(\"Real\", np.round(get_energy(Y[0,:,:,:].detach().cpu())*100), total_step, dataformats=\"HW\")\n",
    "            writer.add_image(\"SR4X\", np.round(get_energy(Y_hat[0,:,:,:].detach().cpu())*100), total_step, dataformats=\"HW\")\n",
    "    return total_step\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    \"\"\"\n",
    "    # 设置数据加载器\n",
    "    # data_loader = get_data_loader()\n",
    "    dataset = TurbulenceDataset(\"data/isotropic1024coarse_full.h5\", 128, 4, 10000)\n",
    "    # 如果这里报错的话，可以把_full改成_test，但是训练出来的模型会不够精确，test只有几张截面数据\n",
    "    data_loader = DataLoader(dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True,\n",
    "                    num_workers=1)\n",
    "    # 设置模型\n",
    "    genG, genF, discX, discY = setup_models(32, 128)\n",
    "    # 设置优化器\n",
    "    genG_opt, genF_opt, discX_opt, discY_opt = setup_optimizers(\n",
    "        genG, genF, discX, discY\n",
    "    )\n",
    "    # 训练模型\n",
    "    total_step = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_step = train_epoch(\n",
    "            data_loader,\n",
    "            genG,\n",
    "            genF,\n",
    "            discX,\n",
    "            discY,\n",
    "            genG_opt,\n",
    "            genF_opt,\n",
    "            discX_opt,\n",
    "            discY_opt,\n",
    "            epoch,\n",
    "            LAMBDA_CYCLE,\n",
    "            total_step\n",
    "        )\n",
    "    \n",
    "        pd.save(genG.state_dict(), f\"saved_models/genG.pdparams\")\n",
    "        pd.save(genF.state_dict(), f\"saved_models/genF.pdparams\")\n",
    "        pd.save(discX.state_dict(), f\"saved_models/discX.pdparams\")\n",
    "        pd.save(discY.state_dict(), f\"saved_models/discY.pdparams\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def setup_optimizers(\n",
    "    genG: nn.Layer, genF: nn.Layer, discX: nn.Layer, discY: nn.Layer\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    设置优化器\n",
    "    \"\"\"\n",
    "    genG_opt = pd.optimizer.Adam(\n",
    "        learning_rate=LEARNING_RATE, parameters=genG.parameters()\n",
    "    )\n",
    "    genF_opt = pd.optimizer.Adam(\n",
    "        learning_rate=LEARNING_RATE, parameters=genF.parameters()\n",
    "    )\n",
    "    discX_opt = pd.optimizer.Adam(\n",
    "        learning_rate=LEARNING_RATE, parameters=discX.parameters()\n",
    "    )\n",
    "    discY_opt = pd.optimizer.Adam(\n",
    "        learning_rate=LEARNING_RATE, parameters=discY.parameters()\n",
    "    )\n",
    "    return genG_opt, genF_opt, discX_opt, discY_opt\n",
    "\n",
    "def setup_models(low_res_size: 32, high_res_size: 128) -> tuple:\n",
    "    \"\"\"\n",
    "    设置模型\n",
    "    \"\"\"\n",
    "    R = int(high_res_size / low_res_size)\n",
    "    # the high resolution generator\n",
    "    genG = GeneratorG([low_res_size, low_res_size])\n",
    "    if os.path.exists(\"saved_models/genG.pdparams\"):\n",
    "        genG.load_dict(pd.load(\"saved_models/genG.pdparams\"))\n",
    "    # the low resolution generator\n",
    "    genF = GeneratorF([high_res_size, high_res_size])\n",
    "    if os.path.exists(\"saved_models/genF.pdparams\"):\n",
    "        genF.load_dict(pd.load(\"saved_models/genF.pdparams\"))\n",
    "    # the low resolution discriminator\n",
    "    discX = DiscriminatorX([low_res_size, low_res_size])\n",
    "    if os.path.exists(\"saved_models/discX.pdparams\"):\n",
    "        discX.load_dict(pd.load(\"saved_models/discX.pdparams\"))\n",
    "    # the high resolution discriminator\n",
    "    discY = DiscriminatorY([high_res_size, high_res_size])\n",
    "    if os.path.exists(\"saved_models/discY.pdparams\"):\n",
    "        discY.load_dict(pd.load(\"saved_models/discY.pdparams\"))\n",
    "    return genG, genF, discX, discY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "把下面的代码取消注释，就会直接训练一个新的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.device.cuda.empty_cache()\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle as pd\n",
    "import paddle.nn as nn\n",
    "# from training import setup_models\n",
    "from utils.dataloader import get_energy, get_rgb\n",
    "\n",
    "genG, _, _, _ = setup_models(32, 128)\n",
    "# load the weights trained\n",
    "genG.set_state_dict(pd.load(\"saved_models/genG.pdparams\"))\n",
    "\n",
    "# create a dataset\n",
    "dataset = TurbulenceDataset(\"data/isotropic1024coarse_test.h5\", 128, 4, 10000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果的可视化\n",
    "\n",
    "在这个格子里面，我们会从数据中随机裁剪出一些(32,32,3)的块，然后用模型超分辨率成4X大小(128,128,3)进行可视化。我们提供了两种绘图的风格，一种是简单的动能图，即：每个像素的动能是$u^2+v^2+w^2$，然后绘制成heatmap；另一种则是将所有的速度分量映射到0-1区间，当作RGB画出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# randomly get 4 pairs of images, use the genG to enlarge the image, show and compare\n",
    "\n",
    "def enlarge(img):\n",
    "    img = pd.to_tensor(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = genG(img)\n",
    "    img = img.squeeze(0)\n",
    "    return img\n",
    "\n",
    "# subplot of 4 * 3 grids\n",
    "fig, axs = plt.subplots(4, 4, figsize=(8,8))\n",
    "axs[0,0].set_title(\"Low Res\")\n",
    "axs[0,1].set_title(\"4X Super Res\")\n",
    "axs[0,2].set_title(\"High Res\")\n",
    "axs[0,3].set_title(\"Error\")\n",
    "for i in range(4):\n",
    "    small, big = dataset[i]\n",
    "    super_ = enlarge(small)\n",
    "\n",
    "    axs[i, 0].imshow(get_energy(small))\n",
    "    axs[i, 1].imshow(get_energy(super_))\n",
    "    axs[i, 2].imshow(get_energy(big))\n",
    "    axs[i, 3].imshow(get_energy(super_ - big))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix： 找到数据里面的均值和极值大小\n",
    "\n",
    "为了将数据标准化到激活函数所在的区域，我们使用了下面的脚本去搜索了(u,v,w)里面的最大值，最后找到大概是2.9 附近，所以如果我们将数据全部除以3，则会得到(-1,1)里面的值，从而可以tanh 激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TurbulenceDataset(\"data/data169495/isotropic1024coarse_10.h5\", 128, 4, 10000, scaler = 1)\n",
    "\n",
    "seen_max = 0\n",
    "for i in range(1000):\n",
    "    _, big = dataset[i]\n",
    "    biggest_value = pd.max(pd.abs(big)).item()\n",
    "    if biggest_value > seen_max:\n",
    "        seen_max = biggest_value\n",
    "        print(seen_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "580cb7770a58b3c615ae2e40de162b0894b5f561592abffb900fb17b36f6079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
